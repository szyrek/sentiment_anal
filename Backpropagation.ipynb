{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmpjfUpi6HWRJkslNjNpvz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/szyrek/sentiment_anal/blob/main/Backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "https://chat.openai.com/share/01b501b8-a0db-4aa5-b04c-ff7cb7903ff3\n"
      ],
      "metadata": {
        "id": "WsJAmdjwjIhu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Backpropagation** is a key algorithm used to train neural networks by calculating the gradient of the loss function with respect to the weights of the network. It's a fundamental concept in deep learning.\n",
        "\n",
        "\n",
        "**Forward Pass:**\n",
        "During the forward pass, input data is fed into the neural network, and the output predictions are calculated by propagating the data forward through the network layers.\n",
        "\n",
        "\n",
        "**Backward Pass (Backpropagation):**\n",
        "During the backward pass (backpropagation), the gradients of the loss function with respect to the parameters (weights and biases) of the network are computed. This is achieved by using the chain rule to recursively propagate the gradients backward through the network layers.\n",
        "\n",
        "\n",
        "**Training a Neural Network using Backpropagation:**\n",
        "\n",
        "1. Forward pass: Compute the output predictions of the neural network given the input data.\n",
        "2. Compute the loss: Calculate the difference between the predicted outputs and the true labels using a loss function.\n",
        "3. Backward pass: Compute the gradients of the loss function with respect to the parameters of the network using backpropagation.\n",
        "4. Update the parameters: Use an optimization algorithm (such as gradient descent) to update the parameters of the network in the direction that minimizes the loss.\n"
      ],
      "metadata": {
        "id": "tsWC98W8fZH4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl8LSixPfMrY",
        "outputId": "07dee171-d448-498b-f821-681cda0afbe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Mean Squared Error: 0.2917\n",
            "Epoch 1000, Mean Squared Error: 0.2496\n",
            "Epoch 2000, Mean Squared Error: 0.2440\n",
            "Epoch 3000, Mean Squared Error: 0.1930\n",
            "Epoch 4000, Mean Squared Error: 0.0695\n",
            "Epoch 5000, Mean Squared Error: 0.0194\n",
            "Epoch 6000, Mean Squared Error: 0.0094\n",
            "Epoch 7000, Mean Squared Error: 0.0059\n",
            "Epoch 8000, Mean Squared Error: 0.0042\n",
            "Epoch 9000, Mean Squared Error: 0.0033\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define sigmoid activation function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Define derivative of sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define neural network architecture\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "\n",
        "# Initialize weights and biases\n",
        "np.random.seed(0)\n",
        "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "biases_hidden = np.zeros((1, hidden_size))\n",
        "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "bias_output = np.zeros((1, output_size))\n",
        "\n",
        "# Forward pass\n",
        "def forward_pass(X):\n",
        "    # Input to hidden layer\n",
        "    hidden_input = np.dot(X, weights_input_hidden) + biases_hidden\n",
        "    hidden_output = sigmoid(hidden_input)\n",
        "\n",
        "    # Hidden to output layer\n",
        "    output_input = np.dot(hidden_output, weights_hidden_output) + bias_output\n",
        "    output = sigmoid(output_input)\n",
        "\n",
        "    return hidden_input, hidden_output, output_input, output\n",
        "\n",
        "# Backpropagation\n",
        "def backpropagation(X, y, hidden_input, hidden_output, output_input, output):\n",
        "    # Compute gradients of output layer\n",
        "    output_error = output - y\n",
        "    output_delta = output_error * sigmoid_derivative(output_input)\n",
        "\n",
        "    # Compute gradients of hidden layer\n",
        "    hidden_error = output_delta.dot(weights_hidden_output.T)\n",
        "    hidden_delta = hidden_error * sigmoid_derivative(hidden_input)\n",
        "\n",
        "    # Compute gradients of weights and biases\n",
        "    d_weights_hidden_output = hidden_output.T.dot(output_delta)\n",
        "    d_bias_output = np.sum(output_delta, axis=0, keepdims=True)\n",
        "    d_weights_input_hidden = X.T.dot(hidden_delta)\n",
        "    d_biases_hidden = np.sum(hidden_delta, axis=0, keepdims=True)\n",
        "\n",
        "    return d_weights_input_hidden, d_biases_hidden, d_weights_hidden_output, d_bias_output\n",
        "\n",
        "# Training\n",
        "# Training\n",
        "def train(X, y, weights_input_hidden, biases_hidden, weights_hidden_output, bias_output, learning_rate, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        # Forward pass\n",
        "        hidden_input, hidden_output, output_input, output = forward_pass(X)\n",
        "\n",
        "        # Backpropagation\n",
        "        d_weights_input_hidden, d_biases_hidden, d_weights_hidden_output, d_bias_output = backpropagation(X, y, hidden_input, hidden_output, output_input, output)\n",
        "\n",
        "        # Update weights and biases\n",
        "        weights_input_hidden -= learning_rate * d_weights_input_hidden\n",
        "        biases_hidden -= learning_rate * d_biases_hidden\n",
        "        weights_hidden_output -= learning_rate * d_weights_hidden_output\n",
        "        bias_output -= learning_rate * d_bias_output\n",
        "\n",
        "        # Compute and print mean squared error\n",
        "        if epoch % 1000 == 0:\n",
        "            mse = np.mean((output - y) ** 2)\n",
        "            print(f\"Epoch {epoch}, Mean Squared Error: {mse:.4f}\")\n",
        "\n",
        "# Generate synthetic data\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Define hyperparameters\n",
        "learning_rate = 0.1\n",
        "epochs = 10000\n",
        "\n",
        "# Define initial weights and biases\n",
        "weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "biases_hidden = np.zeros((1, hidden_size))\n",
        "weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "bias_output = np.zeros((1, output_size))\n",
        "\n",
        "# Train the neural network\n",
        "train(X, y, weights_input_hidden, biases_hidden, weights_hidden_output, bias_output, learning_rate, epochs)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function for testing the trained neural network\n",
        "def test(X_test):\n",
        "    _, _, _, predictions = forward_pass(X_test)\n",
        "    return predictions\n",
        "\n",
        "# Test the trained neural network\n",
        "X_test = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "predictions = test(X_test)\n",
        "\n",
        "# Print predictions\n",
        "print(\"Predictions:\")\n",
        "for i in range(len(X_test)):\n",
        "    print(f\"Input: {X_test[i]}, Prediction: {predictions[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhWoCCGJfX3Y",
        "outputId": "983bb0d7-b4ed-4e6a-f831-71579b9e62f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions:\n",
            "Input: [0 0], Prediction: [0.02739337]\n",
            "Input: [0 1], Prediction: [0.94807488]\n",
            "Input: [1 0], Prediction: [0.94850016]\n",
            "Input: [1 1], Prediction: [0.06653403]\n"
          ]
        }
      ]
    }
  ]
}